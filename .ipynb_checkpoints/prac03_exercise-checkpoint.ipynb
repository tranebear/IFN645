{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Decision Tree\n",
    "\n",
    "---\n",
    "**Written by Hendi Lie (h2.lie@qut.edu.au) and Richi Nayak (r.nayak@qut.edu.au). All rights reserved.**\n",
    "\n",
    "Welcome to the third practical exercise for IFN645. Each exercise sheet contains a number of theoretical and programming exercises, designed to strengthen both conceptual and practical understanding of data mining processes in this unit.\n",
    "\n",
    "To answer conceptual questions, write the answer to each question on a paper/note with your reasoning. For programming exercises, open your iPython console/Jupyter notebook and use Python commands/libraries introduced in each practical to answer the questions. In many cases, you will need to write code to support your conceptual answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Prequisite\n",
    "\n",
    "Perform the following steps before trying the exercises:\n",
    "1. Import pandas as \"pd\" and load the house price dataset into \"df\".\n",
    "2. Print dataset information to refresh your memory.\n",
    "3. Run `preprocess_data` function on the dataframe to perform preprocessing steps discussed last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('datasets/melbourne_house_price.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 24197 entries, 0 to 24196\n",
      "Data columns (total 22 columns):\n",
      "Suburb                24197 non-null object\n",
      "Address               24197 non-null object\n",
      "Rooms                 24197 non-null int64\n",
      "Type                  24197 non-null object\n",
      "Price                 24197 non-null float64\n",
      "Method                24197 non-null object\n",
      "SellerG               24197 non-null object\n",
      "Date                  24197 non-null object\n",
      "Distance              24196 non-null float64\n",
      "Postcode              24196 non-null float64\n",
      "Bedroom2              18673 non-null float64\n",
      "Bathroom              18669 non-null float64\n",
      "Car                   18394 non-null float64\n",
      "Landsize              15946 non-null float64\n",
      "BuildingArea          9609 non-null float64\n",
      "YearBuilt             10961 non-null float64\n",
      "CouncilArea           24194 non-null object\n",
      "Lattitude             18843 non-null float64\n",
      "Longtitude            18843 non-null float64\n",
      "Regionname            24194 non-null object\n",
      "Propertycount         24194 non-null float64\n",
      "Price_above_median    24197 non-null int64\n",
      "dtypes: float64(12), int64(2), object(8)\n",
      "memory usage: 4.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df):\n",
    "    # Q1.4 and Q6.2\n",
    "    df = df.drop(['Address', 'Landsize', 'BuildingArea', 'YearBuilt', 'Price', 'Bedroom2', 'SellerG'], axis=1)\n",
    "    \n",
    "    # Q1.1\n",
    "    cols_miss_drop =['Postcode', 'CouncilArea', 'Regionname', 'Propertycount']\n",
    "    mask = pd.isnull(df['Distance'])\n",
    "\n",
    "    for col in cols_miss_drop:\n",
    "        mask = mask | pd.isnull(df[col])\n",
    "\n",
    "    df = df[~mask]\n",
    "    \n",
    "    # Q1.2\n",
    "    df['Bathroom'].fillna(df['Bathroom'].mean(), inplace=True)\n",
    "    df['Car'].fillna(df['Car'].mean(), inplace=True)\n",
    "    \n",
    "    df['Latitude_nan'] = pd.isnull(df['Lattitude'])\n",
    "    df['Longtitude_nan'] = pd.isnull(df['Longtitude'])\n",
    "    df['Lattitude'].fillna(0, inplace=True)\n",
    "    df['Longtitude'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Q6.1. Change date into weeks and months\n",
    "    df['Sales_week'] = pd.to_datetime(df['Date']).dt.week\n",
    "    df['Sales_month'] = pd.to_datetime(df['Date']).dt.month\n",
    "    df = df.drop(['Date'], axis=1)  # drop the date, not required anymore\n",
    "    \n",
    "    # Q4\n",
    "    df = pd.get_dummies(df)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_prep = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 24194 entries, 0 to 24196\n",
      "Columns: 402 entries, Rooms to Regionname_Western Victoria\n",
      "dtypes: bool(2), float64(7), int64(4), uint8(389)\n",
      "memory usage: 11.2 MB\n"
     ]
    }
   ],
   "source": [
    "df_prep.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"pd.set_option('display.height', 60)\n",
    "pd.set_option('display.max_rows', 60)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.width', 80)\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Partitioning\n",
    "\n",
    "Perform following operations and answer the following questions:\n",
    "1. Describe training, validation and test dataset. What is the purpose for each of these split?\n",
    "    * Training set is used for training the model. \n",
    "    * The validation set is used for testing the model against \"unseen\" data\n",
    "    * test set is used for final accuracy testing\n",
    "2. What is k-fold cross validation? What is the advantage and disadvantage of k-fold CV compared to normal training/test/validation method?\n",
    "    * k-fold cross validation is randomly splitting the data set into k equal sized partition, where the one part will be used for validation while the rest is used for training. advantage is \n",
    "3. What does it mean by *stratification*?\n",
    "    * Stratification ensures same ratio of positive and negative targets in train and test data sets\n",
    "4. What does random state do?\n",
    "    * Random number generator\n",
    "\n",
    "5. Set random state to 0. Split the dataframe into X and Y, then split respective data into training and test set of 70/30 proportion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-b9ec862aaca8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Price_above_median'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Price_above_median'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df2' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = df2['Price_above_median']\n",
    "x = df2.drop(['Price_above_median'], axis=1)\n",
    "\n",
    "rs = 0\n",
    "x_mat = x.as_matrix()\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_mat, y, test_size=0.3, stratify=y, random_state=rs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Decision Tree\n",
    "\n",
    "Perform the following operations and answer the question.\n",
    "1. Import and build a decision tree classifier. Set the random state to 0 to ensure your result is similar with the answers. Fit it against the training data.\n",
    "2. What is the performance of the model against training data? How about against the test data? Do you see any indication of overfitting here?\n",
    "3. What are the top 5 most important features in this model?\n",
    "4. Find the best hyperparameters using GridSearchCV. What is the optimal parameter set? Use the following parameters as initial guidance **criterion** of `gini` or `entropy`, **max depth** of 2-7 and **min_samples_leaf** from 20-60, increment of 10.\n",
    "    \n",
    "5. Visualise the structure of your decision tree. Can you identify characteristics of expensive houses?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# simple decision tree training\n",
    "model = DecisionTreeClassifier(random_state=rs)\n",
    "#model = DecisionTreeClassifier(criterion='gini', random_state=rs, max_depth=2, min_samples_leaf=20)\n",
    "model.fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train accuracy:\", model.score(x_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# grab feature importances from the model and feature name from the original X\n",
    "importances = model.feature_importances_\n",
    "feature_names = x.columns\n",
    "\n",
    "# sort them out in descending order\n",
    "indices = np.argsort(importances)\n",
    "indices = np.flip(indices, axis=0)\n",
    "\n",
    "# limit to 20 features, you can leave this out to print out everything\n",
    "indices = indices[:5]\n",
    "\n",
    "for i in indices:\n",
    "    print(feature_names[i], ':', importances[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'GridSearchCV' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-8047a029cafe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           'min_samples_leaf': range(20, 60, 10)}\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDecisionTreeClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GridSearchCV' is not defined"
     ]
    }
   ],
   "source": [
    "# grid search CV\n",
    "params = {'criterion': ['gini', 'entropy'],\n",
    "          'max_depth': range(2, 7),\n",
    "          'min_samples_leaf': range(20, 60, 10)}\n",
    "\n",
    "cv = GridSearchCV(param_grid=params, estimator=DecisionTreeClassifier(random_state=rs), cv=10)\n",
    "cv.fit(x_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", cv.score(x_train, y_train))\n",
    "print(\"Test accuracy:\", cv.score(x_test, y_test))\n",
    "\n",
    "# test the best model\n",
    "y_pred = cv.predict(x_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# print parameters of the best model\n",
    "print(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion='gini', random_state=rs, max_depth=6, min_samples_leaf=40)\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "print(\"Train accuracy:\", model.score(x_train, y_train))\n",
    "print(\"Test accuracy:\", model.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "# visualize\n",
    "dotfile = StringIO()\n",
    "export_graphviz(model, out_file=dotfile, feature_names=x.columns)\n",
    "graph = pydot.graph_from_dot_data(dotfile.getvalue())\n",
    "graph[0].write_png(\"exercise3.png\") # saved in the following file - will return True if successful"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "\n",
    "When you are finished with all exercise questions, the sample answers are available in the following Github repository. Remember, please try the exercises first before viewing the answers.\n",
    "\n",
    "https://github.com/liehendi11/IFN645_answers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
